\chapter{Predictive models}\label{chap:predictive-models}
This chapter provides a theoretical overview of the prediction models used in the study.

\section{Multiple Linear Regression}\label{sec:models-regression}
Multiple linear regression is a method which assumes that the forecasted variable is dependent on a linear combination of explanatory variables (equation \ref{eq:models-regression}).
\begin{equation}\label{eq:models-regression}
    y_i = {\beta}_0 + \sum_{j = 1}^{p} {{\beta}_j x_{ij}} + {\epsilon}_i
\end{equation}
Meaning of the symbols used in the equation is as follows: $y_i$ is the $i^{th}$ value of the response variable, $x_ij$ is the $i^{th}$ value of the $j^{th}$ explanatory variable with ${\beta}_i$ being the corresponding weight, ${\beta}_0$ is the intercept which can be interpreted as the mean value of the response variable when all of the explanatory variables are equal to 0, $p$ is the number of dependent variables, ${epsilon}_i$ is the error factor expressing the difference between the actual and predicted values of the response variable. 
The goal of regression is to find such values of the parameters $\beta$ that the the sum of squared errors is minimised (equation \ref{eq:models-regression-sse}, symbol $\hat{y_i}$ is the $i^{th}$ actual value of the response variable).
\begin{equation}\label{eq:models-regression-sse}
    SSE = \sum_{i=1}^{n} {(y_i -  \hat{y_i})^2} = \sum_{i = 1}^{n} {{\epsilon}_i}^2
\end{equation}
In order to do so, it is convenient to rewrite equation \ref{eq:models-regression} to a matrix notation \ref{eq:models-regression-matrix}.
\begin{equation}\label{eq:models-regression-matrix}
    \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{bmatrix}
    =
    \begin{bmatrix}
        1 & x_{1, 1} & x_{1, 2} & \hdots & x_{1, p} \\
        1 & x_{2, 1} & x_{2, 2} & \hdots & x_{2, p} \\
        \vdots   & \vdots   & \ddots & \vdots   \\
        1 & x_{n, 1} & x_{n, 2} & \hdots & x_{n, p} \\
    \end{bmatrix}
    \begin{bmatrix}
        {\beta}_0 \\
        {\beta}_1 \\
        {\beta}_2 \\
        \vdots \\
        {\beta}_p
    \end{bmatrix}
    +
    \begin{bmatrix}
        {\epsilon}_1 \\
        {\epsilon}_2 \\
        \vdots \\
        {\epsilon}_n
    \end{bmatrix}
\end{equation}
Now the sum of squared errors can be represented as \ref{eq:models-regression-sse-matrix}.
\begin{equation}\label{eq:models-regression-sse-matrix}
    SSE =
    \bm{\epsilon}^T \bm{\epsilon} = 
    \begin{bmatrix}
        {\epsilon}_1 & {\epsilon}_2 & \hdots & {\epsilon}_n
    \end{bmatrix}
    \begin{bmatrix}
        {\epsilon}_1 \\
        {\epsilon}_2 \\
        \vdots \\
        {\epsilon}_n
    \end{bmatrix}
    = \sum_{i = 1}^{n} {{\epsilon}_i}^2
\end{equation}
\begin{equation}
    \bm{\epsilon}^T \bm{\epsilon} = (\bm{y} - \bm{X}\bm{\beta})^T (\bm{y} - \bm{X}\bm{\beta})
\end{equation}
The optimisation problem can be expressed as:
\begin{equation}\label{eq:models-regression-minimum}
    \min_{\bm{\beta}} (\bm{\epsilon}^T \bm{\epsilon}) = \min_{\bm{\beta}} (\bm{y}^T \bm{y} - 2\bm{\beta}^T \bm{X}^T \bm{y} + \bm{\beta}^T \bm{X}^T \bm{X \beta})
\end{equation}
The minimum of the error function must meet the first-order condition (FOC) which states that the derivative of the error function with respect to the parameters vector $\beta$ must be equal to 0, which is expressed by equation \ref{eq:models-regression-derivative}. In the case of the last component of the right-hand side expression of equation \ref{eq:models-regression-minimum} the product rule was used.
\begin{equation}\label{eq:models-regression-derivative}
    \frac{\partial{(\bm{\epsilon}^T \bm{\epsilon})}}{\partial{\beta}} = 0 - 2 \bm{X}^T \bm{y} + \bm{X}^T\bm{X}\bm{\beta} + \bm{\beta}^T\bm{X}^T\bm{X} = 0
\end{equation}
Since the values of the expressions
\begin{align}
 \bm{X}^T\bm{X}\bm{\beta} \\
 \bm{\beta}^T\bm{X}^T\bm{X}
\end{align}
are scalars and because of the fact that
\begin{equation}
 \bm{X}^T\bm{X}\bm{\beta} = (\bm{\beta}^T\bm{X}^T\bm{X})^T
\end{equation}
equation \ref{eq:models-regression-derivative} might be rewritten as:
\begin{equation}
    \frac{\partial{(\bm{\epsilon}^T \bm{\epsilon})}}{\partial{\beta}} = -2 \bm{X}^T \bm{y} + 2\bm{X}^T\bm{X}\bm{\beta} = 0
\end{equation}
Note that the transpose of a scalar is the same scalar. Now we can express the coefficient vector $\beta$, using the matrix $\bm{X}$ and the vector $\bm{y}$ (equations \ref{eq:model-regression-coefficients} and \ref{eq:model-regression-coefficients-final}), provided that the matrix $\bm{X}^T\bm{X}$ is invertible.

\begin{equation}\label{eq:model-regression-coefficients}
  \bm{X}^T\bm{X}\bm{\beta} = \bm{X}^T \bm{y}
\end{equation}

\begin{equation}\label{eq:model-regression-coefficients-final}
  \bm{\beta} = (\bm{X}^T\bm{X})^{-1} (\bm{X}^T \bm{y})
\end{equation}
Additionally, in order to satisfy the second order condition for a minimum, the matrix $\bm{X}^T\bm{X}$ must be positive definite (equation \ref{eq:model-regression-soc}), which is true if the matrix is non-singular (\cite{LAI2008}).

\begin{equation}\label{eq:model-regression-soc}
    \forall {\bm{v} \in \R^{p}, \bm{v} \neq \bm{0}}: \: \bm{v}(\bm{X}^T\bm{X})\bm{v}^T > 0
\end{equation}
It is worth noting that applicability of multiple regression models is limited by several assumptions that must be met by the data set, e.g.: 
\begin{itemize}
    \item linear relationship between the response variable and predictors,
    \item independence of the response variable values,
    \item normal distribution of the errors with mean equal to zero,
    \item lack of perfect collinearity between the predictors,
    \item lack of correlation between the predictors and error terms,
    \item constant variance of errors all predictor combinations     (\textit{homoscedascicity}),
    \item lack of autocorrelation between the error terms.
\end{itemize}
A more detailed description of the mentioned assumptions can be found in \cite{HOFFMAN2008}.

\subsection*{LASSO regression}
Least Absolute Shrinkage and Selection Operator Regression (LASSO) is a variant of multiple linear regression which includes a variable selection mechanism. It may be beneficial to get rid of unnecessary variables in order to simplify the final model, making it work faster and prevent it from overfitting to the training data. Variables are removed from the model by setting their coefficients ($\bm{\beta}$) in the regression equation to zero. Such an effect is achieved by modifying the formulation of the optimisation problem by adding a term dependent on the coefficients (equation \ref{eq:models-lasso-minimum}). The parameter $\lambda \geq 0$ expresses the strength of the penalty for large coefficients \cite{FONTI2017}.

\begin{equation}\label{eq:models-lasso-minimum}
    \min_{\bm{\beta}} (\bm{\epsilon}^T \bm{\epsilon}) = \min_{\bm{\beta}} ((\bm{y} - \bm{X}\bm{\beta})^T (\bm{y} - \bm{X}\bm{\beta}) + \lambda \sum_{j = 1}^{p} |{\beta}_j|)
\end{equation}

\section{Support Vector Regression}
Another method of fitting a linear function to a set of observations is the Support Vector Regression (SVR). Its goal is find a function $f(x)$ that approximates the available data points in such a way that in all cases the absolute difference between the actual value of the response variable and the value of the fitted function is not higher than $\epsilon$ (an input parameter). An additional condition taken into consideration states that the magnitude of input weights should be as small as possible. The optimisation problem can be formulated as shown in equation \ref{eq:models-svr-optimisation}.
\begin{equation}\label{eq:models-svr-optimisation}
\begin{gathered}
    \text{minimize}\, \frac{1}{2} {\lVert {w} \rVert}^2 \\
    \text{subject to}
    \begin{cases}
        y_i - (\langle \bm{w}, \bm{x_i} \rangle + w_0, & \leq \epsilon \\
        (\langle \bm{w}, \bm{x_i} \rangle + w_0) - y_i, & \leq \epsilon
    \end{cases}
\end{gathered}
\end{equation}
Symbols used in equation \ref{eq:models-svr-optimisation} have the following meaning: $y_i$ is the $i^{th}$ actual value of the response variable, $\bm{w}$ is the vector of input weights, $\bm{x_i}$ is the $i^{th}$ vector of predictor values, $\langle \cdot, \cdot \rangle$ is the dot product, $\epsilon$ is the assumed tolerance margin. 
In some cases the function $f(x)$ may not exist. Because of that additional variables $\xi, {\xi}^*$ representing the exceedance of the tolerance limit are introduced into the problem formulation (equation \ref{eq:models-svr-optimisation-soft}).

\begin{equation}\label{eq:models-svr-optimisation-soft}
\begin{gathered}
    \text{minimize}\, \frac{1}{2} {\lVert {w} \rVert}^2 + C\sum_{i = 1}^{n} ({\xi}_i + {{\xi}_i}^*) \\
    \text{subject to}
    \begin{cases}
        y_i - (\langle \bm{w}, \bm{x_i} \rangle + w_0, & \leq \epsilon + {\xi}_i \\
        (\langle \bm{w}, \bm{x_i} \rangle + w_0) - y_i, & \leq \epsilon + {{\xi}_i}^* \\
        {\xi}_i + {{\xi}_i}^* \geq 0
    \end{cases}
\end{gathered}
\end{equation}
The $C$ coefficient controls the strength of the penalty corresponding to the data points laying outside the tolerance margin. The problem stated in the equation \ref{eq:models-svr-optimisation-soft} is an example of a quadratic programming problem and can be solved using the Lagrange multipliers method. For a more detailed description of the procedure refer to \cite{SMOLA2003}.
\\\\
It is worth noting that an SVR model can be adapted to fitting nonlinear functions transforming the data points using a kernel function and performing the optimisation in the new feature space. A kernel function takes the form of a dot product shown in equation \ref{eq:models-svr-kernel} because the optimisation procedure actually requires calculating the dot products of the input vectors. In this study a radial basis function kernel defined in equation \ref{eq:models-svr-kernel-rbf} was used.

\begin{equation}\label{eq:models-svr-kernel}
K(\bm{x}, \bm{x'}) = \langle \Phi(\bm{x}), \Phi(\bm{x'}) \rangle
\end{equation}

\begin{equation}\label{eq:models-svr-kernel-rbf}
K(\bm{x}, \bm{x'}) = e^{-\gamma {\lVert \bm{x} - \bm{x'} \rVert}^2}
\end{equation}

\section{Neural networks}
An artificial neural network (ANN) is a network comprised of connected processing units called neurons. A neuron is capable of calculating a linear combination of its inputs and an additional parameter called bias which has a similar role to the intercept in linear regression. The output of a neuron is passed as an argument to an activation function for example a sigmoidal function

\begin{equation} \label{eq:models-neural-sigmoid}
g(x) = \frac{1}{1 + e^{-x}}
\end{equation}
Thus the value calculated by the $j^{th}$ neuron can be expressed as in equation \ref{eq:models-neural-neuron}.

\begin{equation} \label{eq:models-neural-neuron}
y_j = g(\sum_{i} w_i x_i + b_j)
\end{equation}
where $w_i$ is the weight of the $i^{th}$ input value $x_i$, $g$ is the activation function and $b_j$ is the bias corresponding to the neuron. Expression \ref{eq:models-neural-neuron} can be visualised as shown in figure \ref{fig:models-neuron}.

\begin{figure}[htp]
\centering
\includegraphics[scale=1.00]{figures/models/neuron.png}
\caption{An artificial neuron}
\label{fig:models-neuron}
\end{figure}
As pointed in \cite{BISHOP1995} a single neuron can be used as a binary classifier in the case of two linearly separable classes. Combining $n$ neurons in a single-layer network allows to classify members of $n$ classes separable with a hyperplane. Two-layered networks are capable of recognising members of a class represented by a convex region. Networks with three layers or more can represent arbitrary decision regions with an arbitrary precision.
It's worth noting that the term n-layered network refers in this case to the number of layers of hidden (other than input and output) neurons.

\subsection{Feedforward networks}
One of the common ways of designing ANNs is to make neurons from layer $n$ have inputs only from layer $n-1$ and outputs passed to layer $n+1$. Neurons in the layer $n$ are fully connected with neurons in the layer $n+1$. Networks that are organised in such a fashion are called feedforward neural networks. One of the benefits of such architecture is the ease of analysis and designing of learning algorithms. 
An example of a feedforward network is shown in Figure \ref{fig:models-feedforward-network}. For the sake of clarity it is assumed that evaluation of the activation function is integrated into the processing units. The solid black circles represent bias inputs.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.6]{figures/models/feedforward-network.png}
\caption{A two-layered feedforward network}
\label{fig:models-feedforward-network}
\end{figure}

\subsection{Network training}\label{ssec:network-training}
The goal of the process of network training is to optimise its classification accuracy by gradually updating the input weights of neurons. The following description is based on \cite{HAN2005}.
Before the training starts input data is divided into two subsets: training and test. Weights of the neuron inputs are set randomly.
The next phase is called forward propagation. Samples from the training set (in the form of floating-point valued vectors) are passed as inputs to the network. The output values of the neurons in the first hidden layer are computed and passed to the next layer etc. The process is repeated until the network outputs are calculated.
Now it is possible to verify network's accuracy by computing the value of a chosen error function, for example a standard sum of squares function
\begin{equation}
SSE = \frac{1}{2}\sum_{i=1}^{n}(y_i - t_i)^2
\end{equation}
where $n$ is the number of outputs and $t_i$ refers to the actual value of the $i^{th}$ output (it is known for the training samples).
In order to adjust the input weights so that the prediction error is decreased it is necessary to perform a step called error back-propagation. It is a process of computing the error corresponding to a specific neuron based on the errors of the neurons from the next layer. Back-propagation starts with calculating errors for each of the outputs, which can be formulated in the form of equations \ref{eq:models-neural-error} and \ref{eq:models-neural-neuron-output}

\begin{equation}\label{eq:models-neural-error}
Err_j = g'(a_j) (t_j - y_j)
\end{equation}
\begin{equation}\label{eq:models-neural-neuron-output}
a_j=\sum_{i} w_{ij} z_i
\end{equation}
where $g'$ is the derivative of the activation function, $t_j$ is the actual value of the $j^{th}$ output, $z_i$ is the $i^{th}$ input of the neuron and $w_{ij}$ is its weight. Fortunately, for the sigmoid function derivative $g'(a_j)$ might be presented as equation \ref{eq:models-neural-sigmoid-derivative} and thus easily computed. Derivation of a generalised expression equivalent to \ref{eq:models-neural-sigmoid-derivative} can be found in \cite{BISHOP1995}.

\begin{equation} \label{eq:models-neural-sigmoid-derivative}
g'(a_j) = y_j (1 - y_j)
\end{equation}
Having found the value of $Err_j$ for the output neurons, it is possible to calculate errors for the last but one layer. The applicable expression takes the form of equation \ref{eq:models-neural-output-error}.

\begin{equation}\label{eq:models-neural-output-error}
Err_j = y_j(1-y_j) \sum_{k} Err_k w_{jk}
\end{equation}
Component $Err_k$ is the error of the k-th neuron in the output layer connected to the neuron $j$. Each time the error value is computed, it is used to update the weights in the algorithm of gradient descent as presented in equations \ref{eq:models-neural-weight-change} and \ref{eq:models-neural-new-weight}.

\begin{equation}\label{eq:models-neural-weight-change}
\Delta w_{ij} = \mu Err_j y_i
\end{equation}
\begin{equation}\label{eq:models-neural-new-weight}
w_{ij} = w_{ij} - \Delta w_{ij}
\end{equation}.
The $\mu$ factor expresses the learning rate. It has been introduced in order to prevent the algorithm from getting stuck in a local optimum
\\
The learning procedure can be stopped after a specific condition has been reached, for example:
\begin{itemize}
	\item the changes of all weights in the last iteration was smaller than a specified threshold; 
	\item the target number of iterations have been executed;
	\item the requirement of maximum percentage of misclassification has been met.
\end{itemize}